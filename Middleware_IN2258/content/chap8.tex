\section{S-Side Architecture}
\redtext{Thread:}
has own Stack, 
Instruction pointer, 
Processor State(Register). 
multiple included in proc.
\redtext{Process:}
running prog. as set of Th.
\redtext{Single‐threaded Pro.:}
One instance of virtual memory
\redtext{Multi‐threaded Pro.:}
One instance
\redtext{+}
\bluetext{th share same memory address space(obj)}
\redtext{Synchronisation:}
sharing same add.(obj), 
Read/Write access synchronized(Mutex, Locks, Semaphores)
\bluetext{Lock:}
\lstinline{Lock l; l.lock();/*critical section,can be Deadlock*/l.unlock();}
\bluetext{Mutal Exclusion:}
Critical sections of Th.s not overlap
\bluetext{Freedom from deadlock, Freedom from starvation}
%
\redtext{Th. State:}
Created
$\rightarrow$
Waiting(Scheduled or unscheduled)
$\leftrightarrow$
Swapped outWaiting(No enough memory), Running
$\rightarrow$
Terminiated, Blocked(Blocking IO)
$\rightarrow$
Waiting(IO finish)
\textbar
Blocked
$\leftrightarrow$
Swapped out andBlocked(No enough memory)
\\
\redtext{API}: 
\lstinline{public class WorkThread extends Thread{public ConnectionHandleThread(Parameter params) @Overridepublic void run(){Computation();//]}
\lstinline{/*use*/Thread t=new WorkThread(params);th.start();Thread.sleep(1000);/*current th*/th.getState();/*NEW, RUNNABLE, BLOCKED, WAITING, TERMINATED, TIMED_WAITING*/th.setPriority(para);th.yield();/*give current use of processor to other th.*/}
\\
\redtext{Amdahl Law:}
max theoretical speedup use multi processors
$T(n)=T(1)*(B+\frac{1}{n}(1-B))$
n:\# processor,
$B\in[0,1]$ is serial,nonparallizable,
speedup:$S(n)=\frac{T(1)}{T(n)}=\frac{1}{T(n)}$
\\
\redtext{Overload}:
\bluetext{1.Transient ov.}
Short spikes in req which ov. the S temporarily
\bluetext{2.Continous ov.}
Incomming exceeds S capacity
\bluetext{3.Methods to tackle ov.}
Refuse req. after certain queue length, Add capacity
\redtext{Concurrency Models}
\bluetext{1.Th‐based conc.}
One req completely handled by single th.;
single th. is responsible until response returned;
If req blocked (DB access), th. blocked
%
%
\bluetext{2.Event‐based conc.}
\redtext{not th. based}
\greentext{
Req in Network/HDD
$\rightarrow$
EventHandler
$\rightarrow$
\# FSM = \# Steps(th.s)
$\rightarrow$
Response}
\btext{--:}
Only non‐blocking(async) I/O operations used,
Code less modular, 
nonreusable,
Complex control flow(FSM, flags), debugging
OS support like async I/O libraries
\greentext{A th. processes req until th. blocked.
When th. blocked, th. continues with the next requests,
Once a th. becomes unblocked, it gets enqueued in the list of ready tasks.}
Scalar code separated in micro tasks.
\redtext{Finate State MachinesFSM:}
Java NIO to track state,
C socket created
$\rightarrow$
\bluetext{th1 accept connection}
register 'C socket event'
\textbar
requests from C socket
$\rightarrow$
\bluetext{th2 lookupDB}
register 'query done event'
\textbar
QueryDone
$\rightarrow$
\bluetext{th3 send 1MB to C}
register 'data sent event'
\textbar
data sent
$\rightarrow$
\bluetext{th4 Cancel connection} 
%
%
\\
\rtext{Single‐threaded S = Iterative S:}
S with only one worker th.
\orangetext{do all also I/O},
\bluetext{Thread‐based conc.}
\greentext{
if inout queue empty then Waits till new req from Network enqueued, 
Dequeue next req.,
single worker th. Compute req result, 
send response to Network},
\orangetext{When blocked due to I/O, th. also blocks(I/O read disk to memory)}
\btext{++:}
Simple, 
Cache Locality(one request in cache)
\btext{--:}
Insufficient use of resources, 
Limited scalability(reject requests if too many)
\rtext{Multi‐threaded S}:
S with \orangetext{multi worker th.}
\bluetext{Workaround: like above} 
as single th. wait \dots block;
OS can schedule non‐blocked th.
\orangetext{get nonblocked th. from pool};
more th. than cores since a certain percentage of I/O is assumed
\btext{++:}
\bluetext{Programming abstraction}
Divide up work to assign to th.
\bluetext{Parallelism}
throughput
\bluetext{Responsiveness}
heavy operations in back with UI still responsive
\bluetext{Blocking I/O}
If blocking I/O, other th. continue to work
\bluetext{Context switching}
Switch costs from th. to another within same process 
\orangetext{cheaper than} 
process‐to‐process switch,
\bluetext{Memory savings}
Th. to share memory, yet utilize multi units of execution
\\
\rtext{One th. per request/Connection:}
scheduled by operations,
\greentext{
Req in Network
$\rightarrow$
InputQueue
$\rightarrow$
Dispatcher
(starts a th. per req,when response sent, th. ends)
$\rightarrow$
Response to Network}
\btext{++:}
Simple 
\btext{--:}
Too many th.
$\rightarrow$
thread‐starvation(little CPU‐time),
cost to Start/Remove th., 
Not ideal for high parallel S
\lstinline{ServerSocket ss = new();ss.bind(new InetSocketAddress("127.0.0.1", port));while(true){Socket clientSocket=ss.accept();Thread t=new ConnectionHandleThread(kv,clientSocket);t.start();}
\btext{+++}
\lstinline{public class ConnectionHandleThread extends Thread{public ConnectionHandleThread(KVStore store, Socket clientSocket)/*[..]*/@Override public void run(){BufferedReader in=clientSocket.getInputStream();PrintWriter out=clientSocket.getOutputStream();String s; while((s=in.readLine())!=null){String res=kv.process(firstLine);out.write(res);out.flush();}
\\
\redtext{Abstracting Th.:}
deadlock \& starvation free
\bluetext{Abstractions:}
\greentext{Thread pool, Event based concurrency, SEDA, Actor model, Reactive programming}
\bluetext{1.Th. pool}
managed set of available th.s,
size as max(eg number of CPU cores),
do tasks in parallel,
Reuses th. for multi tasks
\orangetext{Optimal Size}
depends on \# cores, \# blocking of I/O operations
\greentext{
Request in Network
$\rightarrow$
InputQueue
$\rightarrow$
Scheduler(Thread pool with size)
$\rightarrow$
Response to Network}
\lstinline{ExecutorService es= Executors.newFixedThreadPool(4);while(true){Socket clientSocket=serverSocket.accept();es.submit(new ConnectionHandlerRunnable(kv,clientSocket));}
\bluetext{++:}
RUNNABLE above in One th. per request/Connection
\bluetext{2.Scheduling Algo:}
Most cache efficient to assign task to th.s in pool
\orangetext{Two main paradigms}:Work sharing, Work stealing
\greentext{Work Sharing Algo.:}
When th. created, scheduler moves work of other th. to new th.
\btext{--:}
Comm. between cores, Cache misses
\greentext{Work stealing algo.:}
Under-utilized processors steal work from busy processors
(The migration of threads occurs less frequently with work stealing than with work sharing)
\redtext{++:}
Maintain th. on same CPU(data locality
$\rightarrow$
better cache usage, 
Min comm. between th
%
%
\bluetext{3.Futures:}
result of asyn. computation in th. pool
\lstinline{/*result in future obj*/FutureTask<LinearRegressionResult> future=new<Integer>(new Callable<String>(){public Integer call() {List<Doubles> lr=getDailyTurnover(..);return calcLinearRegression(lr);); threadpool.execute(future);}
%
%
\bluetext{4.SEDA:}
Staged event‐driven architecture, a network of stages connected by event queues
\btext{++:}
Support massive concurrency,
Simplify the construction of services(Provide abstractions),
introspection(Adapt behavior to change load conditions),
Support self‐tuning resource managment,
Server is created from many SEDA stages
%
%
\bluetext{5.Actor:}
No access to shared memory(No locks,no deadlock), 
Scheduled on top of threads
%
%
\bluetext{6.Reactive Programming:}
oriented around data flows and propagation of change(Dependency graph)
\\
\redtext{App S:}
a component‐based product that resides in the middle‐tier of a server‐centric architecture.
It provides middleware services for security and state maintenance, 
along with data access and persistence.
\bluetext{Java:}WildFly, Websphere
\textbar
\btext{EJB}:
Standard component architecture,
Distributed business application(
(Support development, deployment and use of web services),
Write once, run in all application containers
\btext{++:}
Developer not to care Fail‐over, 
Clustering, 
(Distributed) Transaction handling, 
DB, 
Security, 
Deployment